//! Example: Run inference on Akida device
//!
//! Demonstrates the complete inference workflow.

use akida_driver::DeviceManager;
use akida_models::Model;

fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    tracing_subscriber::fmt().with_env_filter("info").init();

    println!("ğŸ§  Akida Inference Demo\n");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");

    // Get model path
    let model_path = std::env::args().nth(1).unwrap_or_else(|| {
        eprintln!("Usage: cargo run --example run_inference -- <path_to_model.fbz>");
        eprintln!("Example: cargo run --example run_inference -- /path/to/model.fbz");
        std::process::exit(1);
    });

    println!("ğŸ“‚ Model: {}\n", model_path);

    // Step 1: Parse model
    println!("1ï¸âƒ£  Parsing model...");
    let model = Model::from_file(&model_path)?;
    println!("   âœ… Parsed: {} layers\n", model.layer_count());

    // Step 2: Discover and open device
    println!("2ï¸âƒ£  Discovering devices...");
    let manager = DeviceManager::discover()?;

    if manager.device_count() == 0 {
        println!("âŒ No devices found!");
        return Ok(());
    }

    let mut device = manager.open_first()?;
    println!("   âœ… Opened device {}\n", device.index());

    // Step 3: Load model to device
    println!("3ï¸âƒ£  Loading model to device...");
    let load_metrics = model.load_to_device(&mut device)?;
    println!("   âœ… Loaded in {:?}\n", load_metrics.duration);

    // Step 4: Prepare input
    println!("4ï¸âƒ£  Preparing input data...");
    let input_size = model.input_size();
    println!("   Input size: {} bytes", input_size);

    // Create dummy input (zeros for demo)
    let input = vec![0u8; input_size];
    println!("   âœ… Input prepared\n");

    // Step 5: Run inference
    println!("5ï¸âƒ£  Running inference...");
    let result = model.infer(&input, &mut device)?;

    // Step 6: Display results
    println!("\nâœ… INFERENCE COMPLETE!\n");
    println!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n");
    println!("ğŸ“Š Inference Metrics:");
    println!("   Input transfer:  {:?}", result.input_transfer_duration);
    println!("   Output transfer: {:?}", result.output_transfer_duration);
    println!("   Total time:      {:?}", result.total_duration);
    println!("   Latency:         {:.1}Âµs", result.latency_us());
    println!(
        "   Throughput:      {:.1} inferences/sec",
        result.throughput_ips()
    );
    println!();

    println!("ğŸ“¦ Data Transfer:");
    println!("   Input bytes:  {}", result.input_bytes);
    println!("   Output bytes: {}", result.output_bytes);
    println!("   Output size:  {} elements\n", result.output.len());

    // Show first few output values
    println!("ğŸ¯ Output Preview (first 10 bytes):");
    print!("   ");
    for (i, &val) in result.output.iter().take(10).enumerate() {
        print!("{:3}", val);
        if i < 9 && i < result.output.len() - 1 {
            print!(" ");
        }
    }
    println!("\n");

    println!(
        "ğŸ‰ Demo complete! Model inference on device {} successful.",
        device.index()
    );
    println!("   Ready for production use!\n");

    Ok(())
}
