/**
 * @file inference.cpp
 * @brief Core Akida inference pipeline implementation
 *
 * This file demonstrates the complete workflow for running inference on Akida hardware:
 * - Device programming with a neural network model
 * - Input tensor preparation and format conversion
 * - Inference execution (enqueue/fetch)
 * - Output dequantization and result extraction
 *
 * Each step is heavily commented to explain the API usage and design decisions.
 */

#include "inference.h"
#include "api/akida/hardware_device.h"
#include "api/akida/dense.h"
#include "api/akida/sparse.h"
#include "api/akida/tensor.h"
#include "api/akida/input_conversion.h"
#include "program.h"  // Generated by convert_model.py - contains model binary
#include <cstdio>
#include <memory>

int run_inference(akida::HardwareDriver* driver) {
    // =========================================================================
    // STEP 1: Create HardwareDevice from driver
    // =========================================================================
    // The HardwareDevice is the main API entry point for controlling Akida hardware.
    // It wraps the low-level driver and provides high-level operations like
    // programming, inference, and learning.

    printf("\n=== Step 1: Creating Hardware Device ===\n");

    std::unique_ptr<akida::HardwareDevice> device = akida::HardwareDevice::create(driver);
    if (!device) {
        fprintf(stderr, "ERROR: Failed to create hardware device\n");
        return 1;
    }

    printf("Device created successfully\n");
    printf("Device description: %s\n", driver->desc());

    // =========================================================================
    // STEP 2: Program the device with the neural network model
    // =========================================================================
    // Programming loads the model onto the Akida chip. The model binary was
    // generated from your .fbz file using convert_model.py.
    //
    // The program() call returns a ProgramInfo object containing critical metadata:
    // - Input/output dimensions and types
    // - Memory requirements
    // - Activation/quantization settings

    printf("\n=== Step 2: Programming Device ===\n");
    printf("Program size: %zu bytes\n", program_len);

    akida::ProgramInfo program_info = device->program(program, program_len);

    // Always validate that programming succeeded
    if (!program_info.is_valid()) {
        fprintf(stderr, "ERROR: Failed to program device - invalid ProgramInfo\n");
        return 1;
    }

    printf("Device programmed successfully\n");

    // Print model metadata for debugging
    auto input_shape = program_info.input_dims();
    auto output_shape = program_info.output_dims();

    printf("Input shape: [%d, %d, %d]\n",
           input_shape[0], input_shape[1], input_shape[2]);
    printf("Output shape: [%d, %d, %d]\n",
           output_shape[0], output_shape[1], output_shape[2]);
    printf("Input format: %s\n",
           program_info.input_is_dense() ? "Dense" : "Sparse");
    printf("Activation enabled: %s\n",
           program_info.activation_enabled() ? "Yes" : "No");

    // =========================================================================
    // STEP 3: Set batch size
    // =========================================================================
    // Batch size determines how many samples are processed per inference call.
    // For simple deployment, batch size 1 is typical.
    // The second parameter (false) disables multi-pass programming.

    printf("\n=== Step 3: Setting Batch Size ===\n");

    if (!device->set_batch_size(1, false)) {
        fprintf(stderr, "ERROR: Failed to set batch size\n");
        return 1;
    }

    printf("Batch size set to 1\n");

    // =========================================================================
    // STEP 4: Prepare input tensor
    // =========================================================================
    // Input data must be wrapped in an akida::Tensor. The format depends on
    // whether the model expects dense or sparse input (check program_info).
    //
    // For this example, we create a simple test pattern. In production, this
    // would be your preprocessed image/audio/sensor data.

    printf("\n=== Step 4: Preparing Input Tensor ===\n");

    // Calculate total input size
    int input_size = input_shape[0] * input_shape[1] * input_shape[2];

    // Create test input data - replace this with your real input
    // For image data, this would typically be RGB or grayscale pixel values
    std::vector<uint8_t> input_data(input_size);
    for (int i = 0; i < input_size; i++) {
        input_data[i] = i % 256;  // Simple test pattern
    }

    printf("Input data prepared: %d elements\n", input_size);

    // Create a Dense tensor view wrapping the input data
    // Note: create_view() does NOT copy data - it creates a lightweight wrapper
    auto dense_input = akida::Dense::create_view(
        input_data.data(),
        akida::Shape{input_shape[0], input_shape[1], input_shape[2]},
        akida::DType::uint8
    );

    if (!dense_input) {
        fprintf(stderr, "ERROR: Failed to create dense input tensor\n");
        return 1;
    }

    // =========================================================================
    // STEP 5: Convert to Sparse format if required
    // =========================================================================
    // Some models expect sparse input. Check program_info.input_is_dense().
    // If the model needs sparse input, convert using akida::conversion::to_sparse()

    std::unique_ptr<akida::Tensor> input_tensor;

    if (!program_info.input_is_dense()) {
        printf("Model requires sparse input - converting...\n");

        // Convert dense to sparse format
        auto sparse_input = akida::conversion::to_sparse(*dense_input);
        if (!sparse_input) {
            fprintf(stderr, "ERROR: Failed to convert input to sparse format\n");
            return 1;
        }

        input_tensor = std::move(sparse_input);
        printf("Converted to sparse format\n");
    } else {
        printf("Model accepts dense input - no conversion needed\n");
        input_tensor = std::move(dense_input);
    }

    // =========================================================================
    // STEP 6: Enqueue input for inference
    // =========================================================================
    // enqueue() sends the input tensor to the Akida device and triggers inference.
    // This is a non-blocking operation - use fetch() to get results.

    printf("\n=== Step 6: Enqueueing Input ===\n");

    if (!device->enqueue(*input_tensor)) {
        fprintf(stderr, "ERROR: Failed to enqueue input tensor\n");
        return 1;
    }

    printf("Input enqueued successfully\n");

    // =========================================================================
    // STEP 7: Fetch inference results
    // =========================================================================
    // fetch() blocks until inference completes and returns the output tensor.
    // Returns nullptr on error.

    printf("\n=== Step 7: Fetching Results ===\n");

    std::unique_ptr<akida::Tensor> output = device->fetch();
    if (!output) {
        fprintf(stderr, "ERROR: Failed to fetch output tensor\n");
        return 1;
    }

    printf("Results fetched successfully\n");

    // =========================================================================
    // STEP 8: Dequantize output if needed
    // =========================================================================
    // If activation is enabled, the output is quantized (int32) and must be
    // dequantized to float using device->dequantize().
    // The output must be in dense format for dequantization.

    std::unique_ptr<akida::Dense> final_output;

    if (program_info.activation_enabled()) {
        printf("\n=== Step 8: Dequantizing Output ===\n");

        // Convert output to dense if it's sparse
        auto dense_output = akida::conversion::as_dense(*output);
        if (!dense_output) {
            fprintf(stderr, "ERROR: Failed to convert output to dense format\n");
            return 1;
        }

        // Dequantize from int32 to float
        final_output = device->dequantize(*dense_output);
        if (!final_output) {
            fprintf(stderr, "ERROR: Failed to dequantize output\n");
            return 1;
        }

        printf("Output dequantized to float\n");
    } else {
        printf("\n=== Step 8: No Dequantization Needed ===\n");

        // Convert to dense for consistent result extraction
        final_output = akida::conversion::as_dense(*output);
        if (!final_output) {
            fprintf(stderr, "ERROR: Failed to convert output to dense format\n");
            return 1;
        }
    }

    // =========================================================================
    // STEP 9: Extract and display results
    // =========================================================================
    // Access the raw output data using data<T>() where T matches the dtype.
    // For classification models, this is typically the class scores/logits.

    printf("\n=== Step 9: Results ===\n");

    auto shape = final_output->shape();
    int output_size = shape.size();

    printf("Output shape: [%d, %d, %d]\n", shape[0], shape[1], shape[2]);
    printf("Output dtype: %s\n",
           final_output->dtype() == akida::DType::float32 ? "float32" : "int32");

    // Print first 10 values (or fewer if output is smaller)
    int values_to_print = std::min(10, output_size);
    printf("First %d output values:\n", values_to_print);

    if (final_output->dtype() == akida::DType::float32) {
        const float* data = final_output->data<float>();
        for (int i = 0; i < values_to_print; i++) {
            printf("  [%d]: %.6f\n", i, data[i]);
        }
    } else {
        const int32_t* data = final_output->data<int32_t>();
        for (int i = 0; i < values_to_print; i++) {
            printf("  [%d]: %d\n", i, data[i]);
        }
    }

    // For classification, find the argmax (predicted class)
    if (output_size > 1) {
        int max_idx = 0;
        float max_val = 0.0f;

        if (final_output->dtype() == akida::DType::float32) {
            const float* data = final_output->data<float>();
            max_val = data[0];
            for (int i = 1; i < output_size; i++) {
                if (data[i] > max_val) {
                    max_val = data[i];
                    max_idx = i;
                }
            }
        } else {
            const int32_t* data = final_output->data<int32_t>();
            int32_t max_val_int = data[0];
            for (int i = 1; i < output_size; i++) {
                if (data[i] > max_val_int) {
                    max_val_int = data[i];
                    max_idx = i;
                }
            }
            max_val = static_cast<float>(max_val_int);
        }

        printf("\nPredicted class: %d (score: %.6f)\n", max_idx, max_val);
    }

    // =========================================================================
    // OPTIONAL: Using the higher-level predict() API
    // =========================================================================
    // For simpler use cases, you can use predict() which combines enqueue/fetch:
    //
    //   auto output = device->predict(*input_tensor);
    //   if (!output) { /* error */ }
    //
    // This is more convenient but gives less control over the pipeline.

    // =========================================================================
    // OPTIONAL: Performance measurement
    // =========================================================================
    // To measure inference latency, use the hardware clock counter:
    //
    //   device->toggle_clock_counter(true);
    //   device->enqueue(*input_tensor);
    //   auto output = device->fetch();
    //   uint64_t cycles = device->read_clock_counter();
    //   device->toggle_clock_counter(false);
    //
    // Convert cycles to time using your device's clock frequency.

    printf("\n=== Inference Complete ===\n");
    return 0;
}
