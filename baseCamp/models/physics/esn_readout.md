# ESN Readout — Thermalization Detector

**Architecture:** InputConv(50→128) → FC(128→1)
**Status:** ✅ VALIDATED — 5,978 live calls on AKD1000 (hotSpring Exp 022)
**Task:** SU(3) lattice QCD plaquette → thermalization flag
**Source:** hotSpring ESN, adapted for Akida by ecoPrimals

---

## The Problem

Lattice QCD Monte Carlo requires thermalization: the Markov chain must reach
equilibrium before observables are collected. Detecting this traditionally means
manual inspection of plaquette history plots, which blocks automation.

The ESN readout solves this: given 50 consecutive plaquette values,
output 1 scalar — the thermalization indicator (0 = still thermalizing, 1 = ready).

---

## Architecture

```
Input: float[50]  (50-step plaquette history, normalized)
  │
  ▼
InputConv(in=50, out=128, kernel=3)   ← reservoir-to-Akida bridge
  │  50 inputs mapped to 128 NPs via 3×1 depthwise conv
  │  weights: int4 (trained from ESN reservoir W_out)
  ▼
FC(in=128, out=1)                     ← linear readout
  │  128 NP activations → 1 output scalar
  │  weights: int4 (least-squares fit W_out quantized)
  ▼
Output: float[1]  (thermalization score, 0.0→1.0)
```

The reservoir (the actual ESN) runs on CPU — hotSpring's `esn::EchoState`.
The Akida chip handles the read-out computation: given reservoir activations,
compute the output. This is the "reservoir offload" pattern:

```
plaquette history → CPU reservoir (sparse W_res) → activations → AKD1000 → output
```

The chip handles the part where inference rate matters most.

---

## Measured Performance

| Metric | Value | Condition |
|--------|-------|-----------|
| Throughput | 18,500 Hz | batch=8 |
| Latency (chip) | 54 µs | single call |
| Latency (end-to-end) | ~650 µs | including PCIe |
| Energy (chip) | 1.4 µJ | at 18.5 KHz |
| Accuracy | 100% | on test set |
| Live calls | 5,978 | hotSpring Exp 022 production |
| False thermalization | 0 | zero early stops |
| Power | ~270 mW | Performance clock |

---

## Weight Format

Weights are stored int4, packed 2 per byte.

```rust
// akida-models: how to reconstruct from training
// Input: W_out from ESN training (128×1 float matrix)
// Output: int4 weights for program_external()

let w_out: Array2<f32> = esn.readout_weights();  // from hotSpring training

// 1. Normalize to [-1, 1] range
let scale = w_out.mapv(|x| x.abs()).fold(f32::NEG_INFINITY, f32::max);
let w_norm = w_out / scale;

// 2. Quantize to int4 [-8, 7]
let w_int4: Array2<i8> = w_norm.mapv(|x| (x * 7.0).round().clamp(-8.0, 7.0) as i8);

// 3. Pack 2 per byte for .fbz program_data
let packed = pack_int4(&w_int4);
```

---

## Cross-Spring References

| Spring | Connection |
|--------|-----------|
| hotSpring | Source of training data (SU(3) plaquette), ESN training, Exp 022 validation |
| neuralSpring | ESN architecture definition, reservoir initialization |
| groundSpring | Uncertainty quantification on thermalization timing |
| rustChip | Hardware execution (this file), VFIO inference |

---

## How to Reproduce

```bash
# Requires AKD1000 hardware + VFIO setup (see specs/DRIVER_SPEC.md)

# Reproduce thermalization detection from Exp 022 data:
cargo run --bin bench_channels -- \
    --model hotspring_esn_readout.fbz \
    --batch 8

# The .fbz file is generated by hotSpring's export pipeline:
# hotSpring/crates/akida-export/src/bin/export_esn_readout.rs
```

---

## Extension Points

1. **Larger reservoir**: 256 or 512 NPs (AKD1000 has 1,000 total)
2. **Deconfinement detector**: same architecture, different output (output 3 phases: confined/deconfined/crossover)
3. **Online adaptation**: use set_variable() to swap W_out weights between β values (Discovery 6 from BEYOND_SDK)
4. **Multi-output**: FC(128→3) for {plaquette, polyakov loop, chiral condensate} simultaneously
