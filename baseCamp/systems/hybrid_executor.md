# Hybrid Executor â€” Software NPU on a Hardware NPU

**Central idea:** The hardware excels at fast, energy-efficient int4 FC operations.
The software excels at arbitrary precision, activations, recurrence, and gradients.
A hybrid executor routes each operation to the substrate best suited to it â€”
transparently, at runtime, from a single unified API.

This is what "a software NPU on a hardware NPU" looks like in concrete terms.

---

## The Problem with Both Extremes

**Pure hardware (SDK approach):**
```
Capabilities:    int4 FC, bounded ReLU, feed-forward only
Limitations:     no tanh, no recurrence, no gradients, no topology change
Energy:          1.4 ÂµJ/inference âœ…
Throughput:      18,500 Hz âœ…
Accuracy:        86.1% on QCD task (tanh gap) âœ…
```

**Pure software (SoftwareBackend):**
```
Capabilities:    f32 tanh, true recurrence, exact gradients, any topology
Limitations:     none architectural â€” just compute budget
Energy:          ~44 mJ/inference âœ— (31,000Ã— worse)
Throughput:      ~800 Hz âœ— (23Ã— worse)
Accuracy:        89.7% on QCD task âœ…
```

**Neither is the answer for production deployment.**

The hybrid executor combines:
- Hardware's energy efficiency and throughput for the operations it does well
- Software's expressiveness for the operations it does better
- A compiler pass that decides at model-load time which operations go where

---

## Architecture

```
User API (single unified call)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       HybridExecutor                                     â”‚
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  OperationRouter (compile-time analysis)                        â”‚    â”‚
â”‚  â”‚  input â†’ [opâ‚€, opâ‚, opâ‚‚, ..., opâ‚™]                            â”‚    â”‚
â”‚  â”‚  each opáµ¢: {type, shape, precision_needed} â†’ substrate choice  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚         â†“ HW route             â†“ SW route                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ AKD1000      â”‚     â”‚  SoftwareBackend                             â”‚  â”‚
â”‚  â”‚ int4 FC ops  â”‚â—„â”€â”€â”€â–ºâ”‚  tanh, sigmoid, gelu, recurrence, gradients  â”‚  â”‚
â”‚  â”‚ SkipDMA      â”‚     â”‚  backprop, arbitrary topology                â”‚  â”‚
â”‚  â”‚ 1.4 ÂµJ/op    â”‚     â”‚  f32/f64 precision                           â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â†“                          â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  OutputMerger: combine HW + SW results â†’ unified output tensor  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Routing Rules

The router assigns each operation to a substrate based on:

| Operation | HW or SW? | Reason |
|-----------|----------|--------|
| FC layer, int4-compatible weights | HW | Native op, 23Ã— faster |
| FC layer, f32 weights | SW | Quantization would lose precision |
| FC layer, large (>1000 nodes) | SW | Exceeds NP budget |
| tanh activation | SW + HW hybrid | HW does linear part, SW applies tanh |
| bounded ReLU | HW | Native op |
| sigmoid activation | SW | Not hardware-native |
| GELU activation | SW | Not hardware-native |
| Recurrent step | HW + SW | HW does W_inÂ·x + W_resÂ·state, SW applies activation |
| Gradient computation | SW | Hardware has no gradient path |
| weight update (gradient step) | SW | Gradients computed in software |
| weight update (mutation) | SWâ†’HW | Computed in SW, applied via set_variable() |
| Multi-head fan-out | HW | SkipDMA â€” native |
| Temporal integration | HW + SW | HW forward pass, SW manages state buffer |

---

## The Critical Insight: tanh-on-Hardware

The biggest quality gap is the activation function.
Hardware: bounded ReLU. Software: tanh. Accuracy gap: 3.6%.

The hybrid executor closes this gap at minimal cost:

```
Standard hardware step:
  input[t] â†’ [W_in Â· input + W_res Â· state] â†’ ReLU â†’ state[t+1]
  Cost: 54 Âµs on hardware

Hybrid tanh step:
  Step 1: input[t] â†’ [W_in Â· input + W_res Â· state] â†’ (linear output) â†’ to host
          Cost: 54 Âµs on hardware (the expensive part)
  Step 2: host applies tanh to 128-float vector
          Cost: < 1 Âµs on CPU (128 tanh operations, vectorized)
  Step 3: host feeds tanh-activated state back as input to next hardware step
          Cost: included in next PCIe transfer

Total: ~55 Âµs (1 Âµs overhead for tanh on host)
Hardware without tanh: 54 Âµs
```

**The tanh activation adds < 2% overhead to hardware inference.**
The accuracy improvement from tanh: +3.6% on QCD task.

The tradeoff is unambiguous: 2% slower, 3.6% more accurate.

```rust
// HybridExecutor tanh-on-hardware step
pub fn step_hybrid_tanh(&mut self, input: &[f32]) -> Result<Vec<f32>> {
    // 1. Run linear transform on hardware (int4, parallel, fast)
    let linear_out = self.hw.infer_linear(input)?;   // hardware FC, no activation

    // 2. Apply tanh on host (trivial cost for reservoir-sized vectors)
    let state_new: Vec<f32> = linear_out.iter().map(|&x| x.tanh()).collect();

    // 3. Store as state for next step (hardware reads it as next "input")
    self.state_buffer = state_new.clone();

    // 4. Apply readout
    let output = self.hw.apply_readout(&state_new)?;
    Ok(output)
}
```

This requires hardware support for "run FC without activation" â€” currently
achievable via `program_external()` with a zero-threshold activation pass.
Planning tracked in `metalForge/experiments/004_HYBRID_TANH.md`.

---

## True Recurrence on Hardware

Hardware is feed-forward. Recurrence requires the host to manage the state
buffer and feed it back. The hybrid executor formalizes this:

```rust
pub struct HybridRecurrentStep {
    hw_device: AkidaDevice,
    state: Vec<f32>,              // previous reservoir state (on host)
    input_dim: usize,
    reservoir_dim: usize,
    activation: Activation,       // tanh, relu, or bounded_relu
}

impl HybridRecurrentStep {
    pub fn step(&mut self, input: &[f32]) -> Result<Vec<f32>> {
        // Concatenate input and previous state into hardware input
        // [input (50 floats)] ++ [state (128 floats)] â†’ 178-float hardware input
        let hw_input: Vec<f32> = input.iter()
            .chain(self.state.iter())
            .copied()
            .collect();

        // Hardware computes: W_in Â· [x ++ state] in one pass (int4, parallel)
        // This is equivalent to W_inÂ·x + W_resÂ·state if we pack the weight matrix
        let pre_activation = self.hw_device.infer_linear(&hw_input)?;

        // Software applies activation (tanh or ReLU, as needed)
        let new_state: Vec<f32> = match self.activation {
            Activation::Tanh => pre_activation.iter().map(|&x| x.tanh()).collect(),
            Activation::BoundedReLU => pre_activation.iter().map(|&x| x.clamp(0.0, 1.0)).collect(),
            Activation::ReLU => pre_activation.iter().map(|&x| x.max(0.0)).collect(),
        };

        // Store updated state for next step
        self.state = new_state.clone();

        Ok(new_state)
    }
}
```

The key: the hardware's `W_in` weight matrix is pre-packed to include both
`W_input_to_hidden` and `W_state_to_hidden` as a single wider matrix.
This is exactly how the existing hardware ESN is deployed â€” the concatenation
trick is already validated in hotSpring Exp 022 âœ….

---

## Online Training Loop (Hardware Forward + Software Backward)

With the hybrid executor, a full online training loop is possible:

```
Forward pass:    hardware (int4, 54 Âµs)
Loss computation: software (f32, ~1 Âµs)
Backward pass:   software (f32, ~100 Âµs for 128-dim reservoir)
Weight update:   software computes Î”w, hardware applies via set_variable() (86 Âµs)

Total per step:  ~240 Âµs â†’ ~4,200 training steps/second

Compare:
  Pure software training:  ~1,250 Âµs forward + backprop â†’ ~800 steps/sec
  GPU training:            ~10 Âµs per step at batch=1, but orders of magnitude more power
```

The hybrid training loop is:
- 5Ã— faster than pure software training
- Still 2Ã— slower than GPU at batch=1
- **90Ã— more energy efficient than GPU** (hardware forward dominates compute cost)

For edge fine-tuning (no cloud, limited power), this is the right tradeoff.

```rust
pub fn online_training_step(
    executor: &mut HybridExecutor,
    input: &[f32],
    target: &[f32],
    learning_rate: f32,
) -> Result<f32> {
    // 1. Forward pass on hardware (54 Âµs)
    let output = executor.forward(input)?;

    // 2. Loss (MSE or cross-entropy, ~1 Âµs)
    let loss = mse(&output, target);

    // 3. Backward pass on software (~100 Âµs for readout layer)
    let grad_out = mse_grad(&output, target);
    let grad_w_out = outer_product(&grad_out, &executor.reservoir_state());

    // 4. Weight update (gradient step)
    let new_w_out = executor.w_out().iter()
        .zip(grad_w_out.iter())
        .map(|(&w, &g)| w - learning_rate * g)
        .collect::<Vec<f32>>();

    // 5. Apply to hardware (86 Âµs set_variable)
    executor.hw_device.set_variable("readout", &new_w_out)?;

    Ok(loss)
}
```

This is the full training loop. Hardware forward + software backward + hardware weight apply.
No Python, no PyTorch, no cloud. Runs on the edge at 4,200 steps/second.

---

## What This System Is Capable Of

A fully realized HybridExecutor enables capabilities that neither substrate
alone can provide:

### 1. Continuous Online Learning with Hardware Speed

Training at 4,200 steps/sec vs 800 steps/sec (pure SW) or 136 gen/sec (evolutionary).
For real-time adaptive systems (acoustic sentinel, physics surrogate), this is
the difference between adapting in 0.2 seconds vs 5.9 seconds.

### 2. tanh ESN on Hardware (Closing the 3.6% Gap)

The accuracy gap between hardware and software shrinks from 3.6% to <0.5%
when the hybrid executor manages activation functions.

At 0.5% gap: hardware and software are effectively identical in accuracy.
Hardware remains 23Ã— faster and 31,000Ã— more energy efficient.
**The only reason to prefer software is architecture search (before committing).**

### 3. Large-Model Chunking

A 2,048-NP reservoir (too large for hardware SRAM) can be chunked:
- Chunk 0 (512 NPs): runs on hardware
- Chunk 1 (512 NPs): runs on software
- Chunk 2 (512 NPs): runs on hardware (second model loaded via multi-tenancy)
- Chunk 3 (512 NPs): runs on software

The executor manages the chunking, inter-chunk routing, and state coherence.
The user sees a single `step()` call.

### 4. Architecture Search Then Compile

```
Phase 1 (software): search over reservoir sizes, activation functions,
                    connectivity patterns, learning rates
                    â†’ ~1,000 candidate architectures evaluated in minutes

Phase 2 (hardware): take the best-performing architecture from Phase 1
                    â†’ quantize weights to int4
                    â†’ compile to FlatBuffer
                    â†’ deploy to hardware
                    â†’ hardware runs it 23Ã— faster, 31,000Ã— more efficiently

Phase 3 (hybrid): use online evolution on hardware to fine-tune for the
                  specific deployment domain
```

This is the complete development pipeline from idea to deployed silicon.
No Python, no TensorFlow, no cloud â€” pure Rust, end to end.

### 5. Adversarial Robustness Testing

```
Train on software (arbitrary perturbations, gradient-based attacks)
  â†“ test attack transferability
Test on hardware (does the attack transfer to int4?)
  â†“ often not â€” int4 quantization disrupts gradient-based attacks
Deploy hardware (naturally robust to software-optimized adversarial examples)
```

int4 quantization as an accidental defense mechanism.
The hybrid executor makes this property measurable.

---

## Implementation Status (Feb 27, 2026)

| Component | Status |
|-----------|--------|
| SoftwareBackend (tanh + true recurrence) | âœ… implemented (`software.rs`) |
| AkidaDevice (hardware forward pass) | âœ… implemented (validated hotSpring Exp 022) |
| `set_variable()` (hardware weight update) | âœ… validated â€” 86 Âµs âœ… |
| `EsnSubstrate` trait | âœ… implemented (`hybrid.rs`) |
| `HybridEsn` struct + `SubstrateSelector` | âœ… implemented (`hybrid.rs`) |
| `EsnWeights` container | âœ… implemented (tanh-trained weight import) |
| Approach B: scale trick math | âœ… Phase 1 validated (`run_experiments --exp 004`) |
| Approach B: hardware dispatch | ðŸ“‹ Phase 2 â€” `metalForge/experiments/004_HYBRID_TANH` |
| Approach A: FlatBuffer threshold override | ðŸ“‹ Phase 2 â€” same experiment |
| Online gradient training loop | ðŸ“‹ `akida-driver 0.2` |
| Large-model chunking | ðŸ“‹ `akida-driver 0.3` |
| Architecture search + compile | ðŸ“‹ `akida-models 0.3` |

**What's working today** (Phase 1):
- `HybridEsn::from_weights()` â€” load tanh-trained weights from hotSpring
- `HybridEsn::step()` â€” PureSoftware mode: CPU f32 + tanh, 800 Hz, correct results
- `HybridEsn::with_hardware_linear()` â€” Approach B emulation: scale trick, non-degenerate
- `HybridEsn::with_hardware_native()` â€” SDK bounded ReLU mode (MetaTF weights)
- `SubstrateSelector::for_weights()` â€” auto-discovers hardware, falls back to software

**What Phase 2 unlocks**:
- Replace `step_linear_emulated()`'s inner matvec with `device.infer()`
- Uncomment hardware discovery in `SubstrateSelector::for_weights()`
- Full 18,500 Hz + tanh accuracy simultaneously, no retraining

**To activate Phase 2** (after `metalForge/experiments/004_HYBRID_TANH` Phase 2):
```bash
# In hybrid.rs: replace the TODO stub in HardwareEsnExecutor::new_linear()
# In hybrid.rs: replace step_linear_emulated() inner matvec with device.infer()
# In hybrid.rs: uncomment the hardware discovery block in SubstrateSelector::for_weights()
```
