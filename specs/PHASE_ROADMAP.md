# Sovereign Driver Roadmap â€” Phase A through E

**Context**: The BrainChip AKD1000/AKD1500 has a Linux PCIe driver
(`akida-pcie-core.c`) that must be rebuilt per kernel version. The AKD1500
datasheet explicitly states "no plans for updates past Linux 6.8."

This roadmap documents the progression from vendor-SDK-dependent to fully
sovereign Rust access â€” from Python wrapper to Rust kernel module.

---

## Phase Overview

```
Phase A: Python SDK â†’ Rust FFI wrapper       [external, not in this repo]
Phase B: C++ Engine â†’ Rust FFI (libakida.so) [external, not in this repo]
Phase C: Direct ioctl/mmap on /dev/akida0    âœ… complete (Feb 26, 2026)
Phase D: Pure Rust VFIO driver               âœ… active (this repo, primary path)
Phase E: Rust akida_pcie kernel module        ðŸ”² queued
```

---

## Phase A â€” Python SDK â†’ Rust FFI Wrapper

**Status**: Complete (external project)
**What it was**: Called into BrainChip's Python `akida` package via PyO3.
**Why it mattered**: First measurement of actual hardware behavior.
**What we learned**: The SDK adds ~15% overhead; hardware is more capable than documented.

---

## Phase B â€” C++ Engine â†’ Rust FFI (libakida.so)

**Status**: Complete (external project)
**What it was**: Called into BrainChip's C++ engine (`libakida.so`) directly,
bypassing the Python layer.
**Why it mattered**: C++ engine exports 1,048 symbols; analyzed with `nm` and `objdump`.
**What we learned**:
- `program_external()` takes raw `program_info` bytes + IOVA address
- `set_variable()` updates weights without full reprogram (~14 ms overhead)
- `SkipDMA` is an internal routing mechanism for FC chain merging
- Three `akida::v1`, `akida::v2`, `akida::pico` hardware variants in engine
- 51-bit threshold SRAM exists (not 4-bit as documented)

---

## Phase C â€” Direct ioctl/mmap on /dev/akida0

**Status**: âœ… Complete â€” February 26, 2026
**What it is**: Opened `/dev/akida*` directly, without SDK or FFI.
Read/write syscalls drive DMA transfers. MMIO via `/dev/mem` for register access.
**Implementation**: `backends/kernel.rs` in this repo.
**Requires**: C `akida_pcie` kernel module loaded, `/dev/akida0` present.
**Results**:
- 37 MB/s DMA throughput (sustained)
- 54 Âµs / 18,500 Hz inference
- Weight mutation without reprogram confirmed
- `program_external()` injection proven â€” full bypass of SDK

---

## Phase D â€” Pure Rust VFIO Driver

**Status**: âœ… Active â€” this repository, primary backend
**What it is**: Linux VFIO/IOMMU provides userspace PCIe device access.
No kernel module in the data path. Pure Rust from `open("/dev/vfio/N")` onward.

**Implementation in this repo**:
- `crates/akida-driver/src/vfio/mod.rs` â€” full VFIO implementation
  - Container, group, device management
  - DMA buffer lifecycle (alloc â†’ mlock â†’ MAP_DMA â†’ use â†’ UNMAP_DMA â†’ free)
  - BAR0 MMIO via `mmap()` on VFIO region
  - Register polling loop with yield for inference completion
  - Inference, model load, reservoir load, power measurement
- `crates/akida-driver/src/mmio.rs` â€” BAR memory mapping
- `crates/akida-cli/src/main.rs` â€” `bind-vfio` and `unbind-vfio` subcommands

**Setup**: One-time per machine (IOMMU enable + vfio-pci bind). After that:
no root, no kernel module, no Python.

**Known gaps**:
- IRQ-based completion (currently polling) â€” `VFIO_DEVICE_SET_IRQS` ready to use
- BAR1 exploration (NP mesh window direct access)
- Scatter-gather DMA for large payloads
- MSI-X interrupt vectors

---

## Phase E â€” Rust akida_pcie Kernel Module

**Status**: ðŸ”² Queued
**What it will be**: A pure Rust replacement for `akida-pcie-core.c` using
the Linux kernel's Rust bindings (`rust/` in the kernel tree, stable since 6.1).

**Why Phase D isn't enough**:
- VFIO requires IOMMU hardware (not all systems have it enabled)
- VFIO requires one-time root setup per machine
- Some embedded/edge deployments need a traditional kernel module interface
- A Rust kernel module can register as a proper PCIe driver (`pci_register_driver`)
  and create `/dev/akida*` without the Python or C++ stack

**Why Phase E beats the C module**:
- Rust kernel module uses stable kernel Rust API (`rust/kernel/` bindings)
- These bindings are committed to not breaking between kernel versions
- The C module must be rebuilt per kernel; the Rust one should not
- Memory safety: no use-after-free, no NULL dereference in the driver
- Single binary: `akida_pcie.ko` â†’ `akida_pcie_rs.ko` â†’ drop-in replacement

**Sketch**:
```rust
// In future crates/akida-kmod/
use kernel::prelude::*;
use kernel::pci::{self, Device as PciDevice};

module! {
    type: AkidaDriver,
    name: "akida_pcie_rs",
    author: "ecoPrimal",
    license: "GPL v2",
}

impl pci::Driver for AkidaDriver {
    fn probe(dev: &pci::Device, _id: &pci::DeviceId) -> Result<Self::Data> {
        dev.enable_device()?;
        dev.set_master();
        dev.request_mem_regions()?;
        // BAR0 mmap, DW eDMA setup, /dev/akidaN creation
        todo!()
    }
}
```

**What BrainChip could do to accelerate this**: Publish the DW eDMA register
map (subset of the DesignWare PCIe eDMA databook) under NDA or open license.
Our probed register map is functional but incomplete; the confirmed offsets
in `crates/akida-chip/src/regs.rs` cover the DMA path but not all features.

---

## Beyond Phase E â€” Future Directions

### 5.1 AKD1500 native support

AKD1500 adds SPI, GPIO, hardware SLEEP, PCIe x2, BGA169 package. None of
these require code changes in the core driver (PCIe x2 is transparent; SPI/GPIO
use separate kernel interfaces). The single change: device ID in `pcie.rs`.

### 5.2 On-chip recurrent execution (Phase F concept)

Discovery 6 confirms weight mutation at ~14 ms. If BrainChip opens the
`akida_learn_on_chip` symbol path, the reservoir update step could execute
on-chip without PCIe round-trip for the weight matrix.

### 5.3 P2P DMA: GPU â†’ NPU without CPU

Both GPU (NVIDIA) and NPU (AKD1000) are PCIe devices. Peer-to-peer DMA would
allow GPU output to flow directly to NPU input without CPU copy:

```
GPU computes (WGSL shader) â†’ GPU result buffer in BAR â†’ P2P DMA â†’ NPU input IOVA
```

Requirements: Both devices in same IOMMU group (or IOMMU bypass), NVIDIA
`nv_peer_mem` or RDMA-capable driver, VFIO BAR peer mapping. This is
documented in detail in `../whitePaper/explorations/GPU_NPU_PCIE.md`.

### 5.4 Rust all the way down

The long-term vision: Rust from application code to silicon. Current stack:

```
Application (Rust) â†’ akida-driver (Rust) â†’ VFIO (Linux kernel C) â†’ IOMMU (hw) â†’ NPU (silicon)
```

After Phase E:
```
Application (Rust) â†’ akida-driver (Rust) â†’ akida_pcie_rs.ko (Rust kernel) â†’ PCIe (hw) â†’ NPU (silicon)
```

The only non-Rust component is the Linux kernel's PCIe infrastructure itself.
As the kernel Rust API matures, that shrinks further.
